
# Is that real? A multifaceted evaluation of the quality of simulated EEG signals for passive BCI

This repository contains the code and data associated with the paper:

**H. Eilts and F. Putze, "Is that real? A multifaceted evaluation of the quality of simulated EEG signals for passive BCI," 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC), Prague, Czech Republic, 2022, pp. 2639-2644.**  
DOI: [10.1109/SMC53654.2022.9945093](https://doi.org/10.1109/SMC53654.2022.9945093)


## Abstract: 

The collection of electroencephalogram (EEG)data is costly, thus it is useful to explore the generation of artificial EEG signals. However, it is difficult to evaluate the quality of these signals. In this work, we generate artificial EEG signals and compare them with real EEG measurements. We analyze the signals in the time and frequency domain and conduct a survey addressed to experts in the field of EEG signal analysis. Finally, we assess whether augmenting EEG data with generated data improves the classification performance. The artificial EEG signals were generated by a progressive Wasserstein Generative Adversarial Network (Wasserstein GAN) with gradient penalty, that was trained on attention recognition data. To investigate the effect of data augmentation, the Shallow Filterbank Common Spatial Pattern network (shallow FBCSPNet) was chosen. The analysis shows that the simulated EEG signals appear realistic in both time and frequency domain. The survey found no significant differences in EEG typical features between real and simulated, but the estimation of noise level tended to be higher for the simulated signals. The data augmentation for the classification resulted in a moderate improvement in accuracy and F-score. Overall, the results show that the quality of the simulated EEG signals is comparable to the quality of the real EEG signals.

-------------------------------

# Ist das echt? Eine Evaluierung der Qualität von simulierten EEG-Signalen

Der von mir geschriebene Code befindet sich in bachelorarbeit/eeggan/Bachelorarbeit.
Um das Programm auszuführen muss nur die Datei main.py gestartet werden. 
Die erforderlichen Bibliotheken sind in der 'requirement.txt'-Datei gelistet.

Die Schritte die in main.py durchlaufen werden, e.g. das Training von GANs oder Klassifikatoren, 
wird durch die Parameter in der Datei 'config.ini' geregelt. Diese befindet sich im obersten Verzeichnis. 
Neben dem Kontrollfluss befinden sich dort auch Parameter die das Training und andere Schritte beeinflussen (e.g. Anzahl Epochen beim Training).
Zum Testen des Programms gibt es einen Testmodus, der in der Konfigurationsdatei aktiviert werden kann. Dieser beschränkt die Anzahl
der Daten, die Anzahl der Epochen etc., um grundlegende Funktionalitäten zu überprüfen. 

Insgesamt gibt es 8 Schritte in dieser Pipeline (Abschnitt [PIPELINE_STEPS] in config.ini):

1. Erzeugung der Datensätze
2. Training der GANs
3. Training der Klassifikatoren auf den nicht-augmentieren Datensätzen
4. Training der Klassifikatoren auf den augmentieren Datensätzen
5. Erzeugung der Fragen/Bilder für die Umfrage
6. Die Evaluierung der GANs
7. Die Evaluierung der Klassifikatoren
8. Die Evaluierung der Umfrage

Um einen Schritt zu aktivieren muss der jeweilige Wert auf True gesetzt werden. 

Die Daten für das Training der Modelle sind hier zu finden: 
https://seafile.zfn.uni-bremen.de/d/a5a060bf6240466f983e/

Das Programm erwartet, dass sich der Ordner mit den Daten ('Data') eine Ebene über dem Hauptverzeichnis des Projekts befindet. 

Die Ergebnisse der Evaluierung werden in einem Ordner 'results' gespeichert, der automatisch erzeugt wird. 
Dieser befindet sich dann ebenfalls eine Ebene über dem Hauptverzeichnis des Projekts.

Zum Reproduzieren der Ergebnisse der Bachelorarbeit ist der vorgefertigte  Ordner 'results' herunterzuladen, welcher den neu erzeugten ersetzt. Dieser beinhaltet die vortrainierten Modelle und auch die vorberechneten Ergebnisse der Evaluierung in Form von gespeicherten Dictionaries ('result_dict.npy' bei den Klassifikatoren und 'scores_dict.npy' bei den GANs). 
Wenn die Evaluierung der Klassifikatoren und/oder GANs mit diesen Daten geschehen soll, dann sollte in der config.ini sichergestellt werden,
dass im Abschnitt "Evaluation" die Variablen 'c_calc_metrics' und 'gan_calc_metrics' auf False gesetzt sind.
Wenn diese auf True gesetzt sind werden die Ergebnisse komplett neu berechnet und die dictionaries im Results-Ordner überschrieben. 
Weiter ist Vorsicht geboten, wenn das Training der GANs, Klassifikatoren, etc. erneut ausgeführt wird, da diese alte Ergebnisse überschreiben.






